{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, History, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.layers import BatchNormalization, Input, Reshape, Dense, LSTM, Dropout, Permute\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from timeit import default_timer as timer\n",
    "from src.train.utils import build_numpy, build_generator, fetch_file_list_pn, BATCH_SIZE, build_numpy, \\\n",
    "    fetch_file_list\n",
    "\n",
    "\n",
    "X_COLUMNS = 1\n",
    "Y_COLUMNS = 0\n",
    "BATCH_SIZE = 128\n",
    "LR = 5e-4\n",
    "# number of samples. A tiny number of samples for fast test\n",
    "# For real training should reflect number of samples in whole training set\n",
    "NUM_TRAIN_SAMPLES = BATCH_SIZE * 2\n",
    "NUM_VALI_SAMPLES = BATCH_SIZE * 2\n",
    "\n",
    "EPOCHS = 5\n",
    "np.random.seed(100)\n",
    "\n",
    "TRAINING_DATA_DIR = '/Users/lucindazhao/strava/ml-local/trainData/'\n",
    "VALIDATION_DATA_DIR = '/Users/lucindazhao/strava/ml-local/validationData/'\n",
    "TRAIN_PORTION = 10.1\n",
    "VALIDATION_PORTION = 10.1\n",
    "MODEL_CHECKPOINT = '/Users/lucindazhao/strava/ml-local/snapshots/v1/'\n",
    "LOG_DIR = '/Users/lucindazhao/strava/ml-local/logs/'\n",
    "IS_CSV = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lr):\n",
    "    # build CNN network\n",
    "    # model definition\n",
    "    modelGraph = Sequential()\n",
    "    # make sure to define input layer\n",
    "    input1 = Input(shape=(None,))\n",
    "    # get N * 26 input matrix\n",
    "    reshape2 = Reshape((26, -1), name=\"reshape2\")\n",
    "\n",
    "    # get 26 * N input matrix\n",
    "    transpose = Permute((2, 1), name=\"transpose\")\n",
    "    layer1 = LSTM(units=200, return_sequences = True, name=\"lstm1\")\n",
    "    layer2 = LSTM(units=200, return_sequences = False, name=\"lstm2\")\n",
    "    dropout = Dropout(0.3, name=\"dropout\")\n",
    "    dense_layer = Dense(units=2, activation=\"softmax\", kernel_regularizer=regularizers.l2(1e-3))\n",
    "    modelGraph.add(input1)\n",
    "    modelGraph.add(reshape2)\n",
    "    modelGraph.add(transpose)\n",
    "    modelGraph.add(layer1)\n",
    "    modelGraph.add(layer2)\n",
    "    modelGraph.add(dropout)\n",
    "    modelGraph.add(dense_layer)\n",
    "    \n",
    "    print(modelGraph.summary())\n",
    "    return 'lstm_200hidden_2layer_lr' + str(lr), modelGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape2 (Reshape)           (None, 26, None)          0         \n",
      "_________________________________________________________________\n",
      "transpose (Permute)          (None, None, 26)          0         \n",
      "_________________________________________________________________\n",
      "lstm1 (LSTM)                 (None, None, 200)         181600    \n",
      "_________________________________________________________________\n",
      "lstm2 (LSTM)                 (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 502,802\n",
      "Trainable params: 502,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('lstm_200hidden_2layer_lr0.01',\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x14e319a58>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_lstm(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Mini model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Files: 10\n",
      "Number of Files: 10\n",
      "Number of Positive Training Windows: 192986.0\n",
      "Number of Negative Training Windows: 2547081.0\n"
     ]
    }
   ],
   "source": [
    "train_file_list = fetch_file_list(data_dir=TRAINING_DATA_DIR, portion=TRAIN_PORTION)\n",
    "tg = build_numpy(file_list=train_file_list, num_samples=None, xcolumns=X_COLUMNS, shuffle=False,\n",
    "                 ycolumns=Y_COLUMNS, ytx=None, is_csv=IS_CSV)\n",
    "\n",
    "val_file_list = fetch_file_list(data_dir=VALIDATION_DATA_DIR, portion=VALIDATION_PORTION)\n",
    "vg = build_numpy(file_list=val_file_list, num_samples=None, xcolumns=X_COLUMNS, ycolumns=Y_COLUMNS, ytx=None,\n",
    "                 skip_header=1, shuffle=False, is_csv=IS_CSV)\n",
    "\n",
    "count = np.sum(tg[1], axis=0)\n",
    "print(\"Number of Positive Training Windows: {}\".format(count[0]))\n",
    "print(\"Number of Negative Training Windows: {}\".format(len(tg[1]) - count[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape2 (Reshape)           (None, 26, None)          0         \n",
      "_________________________________________________________________\n",
      "transpose (Permute)          (None, None, 26)          0         \n",
      "_________________________________________________________________\n",
      "lstm1 (LSTM)                 (None, None, 200)         181600    \n",
      "_________________________________________________________________\n",
      "lstm2 (LSTM)                 (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 502,802\n",
      "Trainable params: 502,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "lstm_200hidden_2layer_lr0.0005_small\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape2 (Reshape)           (None, 26, None)          0         \n",
      "_________________________________________________________________\n",
      "transpose (Permute)          (None, None, 26)          0         \n",
      "_________________________________________________________________\n",
      "lstm1 (LSTM)                 (None, None, 200)         181600    \n",
      "_________________________________________________________________\n",
      "lstm2 (LSTM)                 (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 502,802\n",
      "Trainable params: 502,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 2740067 samples, validate on 1352152 samples\n",
      "Epoch 1/5\n",
      "2740067/2740067 [==============================] - 913s 333us/sample - loss: 0.0925 - accuracy: 0.7267 - val_loss: 0.0887 - val_accuracy: 0.7670\n",
      "Epoch 2/5\n",
      "2740067/2740067 [==============================] - 982s 358us/sample - loss: 0.0866 - accuracy: 0.7620 - val_loss: 0.0859 - val_accuracy: 0.7831\n",
      "Epoch 3/5\n",
      "2740067/2740067 [==============================] - 1025s 374us/sample - loss: 0.0841 - accuracy: 0.7756 - val_loss: 0.0847 - val_accuracy: 0.7895\n",
      "Epoch 4/5\n",
      "2739840/2740067 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.7858"
     ]
    }
   ],
   "source": [
    "for lr in [5e-4, 1e-4, 1e-3, 1e-2]:\n",
    "    model_pair = build_lstm(lr)\n",
    "    key = model_pair[0] + '_small'\n",
    "    model = model_pair[1]\n",
    "    print(key)\n",
    "    print(model.summary())\n",
    "    file_path = MODEL_CHECKPOINT + key + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    history = History()\n",
    "    log_dir = LOG_DIR + key + '/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensor_board = TensorBoard(log_dir, histogram_freq=5,\n",
    "                               write_grads=False, write_graph=False)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                          patience=4, min_lr=1e-5)\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min', period=5)\n",
    "\n",
    "    # omit reduce_lr for now\n",
    "    callbacks_list = [history, tensor_board, checkpoint]\n",
    "\n",
    "    adam_wn = Adam(lr=lr)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=adam_wn, metrics=['accuracy'])\n",
    "    model.fit(x=tg[0], y=tg[1], validation_data=vg, batch_size=BATCH_SIZE,\n",
    "      epochs=EPOCHS, verbose=1, shuffle=False,\n",
    "      callbacks=callbacks_list, class_weight={0: 0.1, 1: 1.0})\n",
    "    model.save(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
